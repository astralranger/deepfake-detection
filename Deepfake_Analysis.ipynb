{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Deepfake Detection Project**\n",
        "\n",
        "## **1. About the Project**\n",
        "The **Deepfake Detection Project** is designed to distinguish between **real** and **manipulated** facial images using deep learning. The project employs:\n",
        "\n",
        "- **MobileNetV2-based classifier** for binary classification (Real/Fake).\n",
        "- **Autoencoder** for anomaly detection.\n",
        "- **Grad-CAM** for explainability (visual heatmaps).\n",
        "- **Face landmarks detection** and **blur analysis** for enhanced detection.\n",
        "- **Gradio UI** for real-time deepfake detection on images and videos.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Workflow Overview**\n",
        "The project consists of the following stages:\n",
        "\n",
        "### **ðŸ“Œ Data Preparation and EDA**\n",
        "- Dataset: `https://www.kaggle.com/competitions/deepfake-detection-challenge`\n",
        "- Exploratory Data Analysis (EDA) includes:\n",
        "  - Dataset distribution visualization\n",
        "  - Sample image previews\n",
        "  - Metadata exploration\n",
        "\n",
        "### **ðŸ“Œ Model Training**\n",
        "1. **Autoencoder Training**\n",
        "   - Learns latent representations of real images.\n",
        "   - Uses **Mean Squared Error (MSE)** loss.\n",
        "   - Architecture: CNN-based encoder-decoder.\n",
        "\n",
        "2. **Deepfake Classifier Training**\n",
        "   - **MobileNetV2** with a modified final classification layer.\n",
        "   - Uses **Cross-Entropy Loss**.\n",
        "   - Optimized with **Adam optimizer**.\n",
        "\n",
        "### **ðŸ“Œ Feature Extraction**\n",
        "- **Grad-CAM**: Highlights important image regions.\n",
        "- **Face Landmarks**: Uses dlib for key feature extraction.\n",
        "- **Blur Analysis**: Uses Laplacian variance to detect artifacts.\n",
        "- **Anomaly Detection**: Autoencoder reconstruction errors.\n",
        "\n",
        "### **ðŸ“Œ Deployment via Gradio**\n",
        "- **Image-Based Detection**\n",
        "- **Video-Based Detection**\n",
        "- **Tabbed Interface for User Interaction**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8-baIZn2xMrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision gradio dlib opencv-python matplotlib numpy captum"
      ],
      "metadata": {
        "id": "fE1m0j9Zv4de"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_deepfake_detector.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import mobilenet_v2\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "\n",
        "# Set device (GPU if available, else CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define transformations for image preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((96, 96)),  # Resize images to 96x96\n",
        "    transforms.ToTensor(),        # Convert image to tensor\n",
        "    transforms.Normalize([0.5], [0.5])  # Normalize to [-1,1] range\n",
        "])\n",
        "\n",
        "# Load dataset\n",
        "train_path = \"/content/drive/MyDrive/G15/archive/real_and_fake_face/train\"\n",
        "test_path = \"/content/drive/MyDrive/G15/archive/real_and_fake_face/test\"\n",
        "\n",
        "train_dataset = ImageFolder(root=train_path, transform=transform)\n",
        "test_dataset = ImageFolder(root=test_path, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Load MobileNetV2 model\n",
        "model = mobilenet_v2(pretrained=True)\n",
        "\n",
        "# Modify classifier to match 2-class classification (Real vs Fake)\n",
        "model.classifier[1] = nn.Linear(model.last_channel, 2)\n",
        "model = model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    for imgs, labels in train_loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
        "# Save trained model\n",
        "torch.save(model.state_dict(), \"deepfake_detector.pth\")\n",
        "print(\"Model saved as 'deepfake_detector.pth'\")\n"
      ],
      "metadata": {
        "id": "IMD-qS8owK3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_autoencoder.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Define the Autoencoder model for unsupervised anomaly detection\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        # Encoder: reduces image dimensionality\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, 3, stride=2, padding=1),  # 16x48x48\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, 3, stride=2, padding=1),  # 32x24x24\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 3, stride=2, padding=1),  # 64x12x12\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # Decoder: reconstructs original image\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),  # 32x24x24\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),  # 16x48x48\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),   # 1x96x96\n",
        "            nn.Sigmoid()  # Output in range [0, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Image transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.Resize((96, 96)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Load dataset\n",
        "dataset_path = \"/content/drive/MyDrive/G15/archive/real_and_fake_face\"\n",
        "dataset = ImageFolder(root=dataset_path, transform=transform)\n",
        "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Initialize model, loss, optimizer\n",
        "autoencoder = Autoencoder().to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for imgs, _ in loader:\n",
        "        imgs = imgs.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = autoencoder(imgs)\n",
        "        loss = criterion(outputs, imgs)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(loader):.4f}\")\n",
        "\n",
        "# Save the trained autoencoder model\n",
        "torch.save(autoencoder.state_dict(), \"autoencoder.pth\")\n",
        "print(\"Autoencoder model saved as 'autoencoder.pth'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItdXP0Zyh-DP",
        "outputId": "8efb820b-11bc-430a-f477-fa9f7db0c8fc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.0451\n",
            "Epoch [2/10], Loss: 0.0108\n",
            "Epoch [3/10], Loss: 0.0057\n",
            "Epoch [4/10], Loss: 0.0046\n",
            "Epoch [5/10], Loss: 0.0040\n",
            "Epoch [6/10], Loss: 0.0037\n",
            "Epoch [7/10], Loss: 0.0034\n",
            "Epoch [8/10], Loss: 0.0034\n",
            "Epoch [9/10], Loss: 0.0031\n",
            "Epoch [10/10], Loss: 0.0031\n",
            "Autoencoder model saved as 'autoencoder.pth'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# deepfake_detector.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import mobilenet_v2\n",
        "import numpy as np\n",
        "import cv2\n",
        "import gradio as gr\n",
        "import dlib\n",
        "from PIL import Image\n",
        "from captum.attr import GuidedGradCam\n",
        "\n",
        "#######################################\n",
        "# Load MobileNetV2-based Deepfake Detector\n",
        "#######################################\n",
        "model = mobilenet_v2(pretrained=True)\n",
        "model.classifier[1] = nn.Linear(model.last_channel, 2)\n",
        "model.load_state_dict(torch.load('deepfake_detector.pth', map_location=torch.device('cpu')))\n",
        "model.eval()\n",
        "\n",
        "# Grad-CAM layer\n",
        "target_layer = model.features[-1]\n",
        "\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
        "\n",
        "#######################################\n",
        "# Image Preprocessing Function\n",
        "#######################################\n",
        "def preprocess_image(image):\n",
        "    transform_fn = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize((96, 96)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5], [0.5])\n",
        "    ])\n",
        "    return transform_fn(image).unsqueeze(0)\n",
        "\n",
        "#######################################\n",
        "# Grad-CAM for Fake Region Detection\n",
        "#######################################\n",
        "def grad_cam(image):\n",
        "    image_tensor = preprocess_image(image)\n",
        "    guided_gc = GuidedGradCam(model, target_layer)\n",
        "    cam = guided_gc.attribute(image_tensor, target=1).detach().numpy()[0]\n",
        "    cam = np.transpose(cam, (1, 2, 0))\n",
        "    cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8) * 255\n",
        "    return cam.astype(np.uint8)\n",
        "\n",
        "#######################################\n",
        "# Blur Detection (Deepfake Artifacts)\n",
        "#######################################\n",
        "def detect_blur(image):\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    variance = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
        "    return \"Blurry\" if variance < 100 else \"Sharp\"\n",
        "\n",
        "#######################################\n",
        "# Multi-Face Deepfake Detection\n",
        "#######################################\n",
        "def detect_landmarks(image):\n",
        "    img_copy = image.copy()\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    faces = detector(gray)\n",
        "    results = []\n",
        "    for face in faces:\n",
        "        landmarks = predictor(gray, face)\n",
        "        for i in range(68):\n",
        "            x, y = landmarks.part(i).x, landmarks.part(i).y\n",
        "            cv2.circle(img_copy, (x, y), 1, (0, 255, 0), -1)\n",
        "        results.append((face, img_copy))\n",
        "    return results if results else [(None, img_copy)]\n",
        "\n",
        "#######################################\n",
        "# Predict Function with Confidence Score\n",
        "#######################################\n",
        "def predict(image):\n",
        "    processed = preprocess_image(image)\n",
        "    with torch.no_grad():\n",
        "        output = model(processed)\n",
        "        probabilities = torch.nn.functional.softmax(output, dim=1)\n",
        "        confidence = probabilities[0][1].item() * 100\n",
        "    result = f\"Real ({confidence:.2f}%)\" if confidence > 50 else f\"Fake ({100-confidence:.2f}%)\"\n",
        "\n",
        "    gradcam_output = grad_cam(image)\n",
        "    landmarks_output = detect_landmarks(image.copy())[0][1]\n",
        "    blur_result = detect_blur(image)\n",
        "\n",
        "    return result, gradcam_output, landmarks_output, blur_result\n",
        "\n",
        "#######################################\n",
        "# Video Frame-by-Frame Analysis\n",
        "#######################################\n",
        "def analyze_video(video):\n",
        "    cap = cv2.VideoCapture(video)\n",
        "    frames = []\n",
        "    results = []\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        result, _, _, _ = predict(frame)\n",
        "        frames.append(frame)\n",
        "        results.append(result)\n",
        "    cap.release()\n",
        "    return results\n",
        "\n",
        "#######################################\n",
        "# Gradio Interfaces\n",
        "#######################################\n",
        "image_ui = gr.Interface(\n",
        "    fn=predict,\n",
        "    inputs=[gr.Image(type='numpy', label=\"Input Image\")],\n",
        "    outputs=[\n",
        "        gr.Label(label=\"Classification\"),\n",
        "        gr.Image(type='numpy', label=\"Grad-CAM\"),\n",
        "        gr.Image(type='numpy', label=\"Face Landmarks\"),\n",
        "        gr.Label(label=\"Blur Detection\")\n",
        "    ],\n",
        "    title=\"Enhanced Deepfake Image Detector\",\n",
        "    description=\"Upload an image to check if it's real or fake with confidence score and visual analysis.\"\n",
        ")\n",
        "\n",
        "video_ui = gr.Interface(\n",
        "    fn=analyze_video,\n",
        "    inputs=gr.Video(label=\"Input Video\")\n",
        ",\n",
        "    outputs=gr.Textbox(label=\"Frame-by-Frame Deepfake Analysis\"),\n",
        "    title=\"Deepfake Video Detector\",\n",
        "    description=\"Upload a video to analyze each frame for deepfakes.\"\n",
        ")\n",
        "\n",
        "dashboard = gr.TabbedInterface([image_ui, video_ui], [\"Image Detection\", \"Video Analysis\"])\n",
        "dashboard.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "4QiqofOpp_Ee",
        "outputId": "a9f50f5a-c981-439c-ffbf-229593dd6589"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-3c39ec891688>:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('deepfake_detector.pth', map_location=torch.device('cpu')))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://7e90a2489dd843ed1c.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://7e90a2489dd843ed1c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}